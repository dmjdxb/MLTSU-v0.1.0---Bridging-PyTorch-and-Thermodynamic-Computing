# MLTSU Implementation Guide

## Project: PyTorch â†’ Thermodynamic Sampling Unit Bridge

This guide provides step-by-step instructions for building MLTSU, a PyTorch-native framework that bridges mainstream deep learning with thermodynamic computing hardware (TSUs/p-bits).

## ðŸŽ¯ Mission Statement

Create the canonical bridge from PyTorch to thermodynamic hardware, starting with simulators and becoming hardware-ready for real TSU devices. This is not about magically training GPT-4 at 1000Ã— lower energy, but about designing APIs and reference implementations that let thermodynamic hardware slot into modern LLM training and inference.

## ðŸ“‹ Prerequisites

- Python 3.9+
- CUDA-capable GPU (optional but recommended)
- Basic understanding of PyTorch, JAX, and statistical mechanics
- ~8GB RAM for simulations

## ðŸ—ï¸ Project Structure

```
mltsu/
â”œâ”€â”€ mltsu/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ tsu_core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ interfaces.py      # TSUBackend protocol
â”‚   â”‚   â””â”€â”€ utils.py           # Common utilities
â”‚   â”œâ”€â”€ tsu_jax_sim/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ state.py           # p-bit state management
â”‚   â”‚   â”œâ”€â”€ energy_models.py   # Ising models
â”‚   â”‚   â”œâ”€â”€ sampler.py         # Sampling algorithms
â”‚   â”‚   â””â”€â”€ backend.py         # JAXTSUBackend
â”‚   â”œâ”€â”€ tsu_pytorch/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ bridge.py          # PyTorch-JAX interop
â”‚   â”‚   â”œâ”€â”€ binary_layer.py    # TSUBinaryLayer
â”‚   â”‚   â”œâ”€â”€ noise.py           # TSUGaussianNoise
â”‚   â”‚   â”œâ”€â”€ attention.py       # ThermodynamicAttention
â”‚   â”‚   â”œâ”€â”€ negatives.py       # TSUNegativeSampler
â”‚   â”‚   â””â”€â”€ memory.py          # Future: ThermodynamicMemory
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tiny_thermo_lm.py  # Demo transformer
â”‚   â”‚   â””â”€â”€ mnist_tsu_diffusion.py
â”‚   â””â”€â”€ streamlit/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ ising_app.py       # Interactive playground
â”‚       â”œâ”€â”€ attention_viz.py   # Attention visualization
â”‚       â””â”€â”€ lm_playground.py    # LM comparison
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_core.py
â”‚   â”œâ”€â”€ test_jax_sim.py
â”‚   â”œâ”€â”€ test_pytorch_bridge.py
â”‚   â””â”€â”€ test_models.py
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic_ising.ipynb
â”‚   â”œâ”€â”€ thermodynamic_attention.ipynb
â”‚   â””â”€â”€ diffusion_demo.ipynb
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ sampling_speed.py
â”‚   â””â”€â”€ energy_comparison.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ api.md
â”‚   â””â”€â”€ theory.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

## ðŸš€ Implementation Phases

### Phase 1: Core Infrastructure (Week 1-2)

#### Step 1.1: Initialize Project

```bash
# Create project structure
mkdir -p mltsu/{mltsu/{tsu_core,tsu_jax_sim,tsu_pytorch,models,streamlit},tests,examples,benchmarks,docs}

# Initialize git
git init
```

#### Step 1.2: Create Requirements

```python
# requirements.txt
torch>=2.0.0
jax>=0.4.0
jaxlib>=0.4.0
numpy>=1.24.0
streamlit>=1.25.0
matplotlib>=3.6.0
tqdm>=4.65.0
pytest>=7.3.0
black>=23.0.0
mypy>=1.3.0
jupyter>=1.0.0
plotly>=5.14.0
scipy>=1.10.0
```

#### Step 1.3: Setup Configuration

```toml
# pyproject.toml
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "mltsu"
version = "0.1.0"
description = "PyTorch to Thermodynamic Sampling Unit Bridge"
requires-python = ">=3.9"
dependencies = [
    "torch>=2.0.0",
    "jax>=0.4.0",
    "numpy>=1.24.0",
]

[project.optional-dependencies]
dev = ["pytest", "black", "mypy"]
viz = ["streamlit", "plotly", "matplotlib"]
```

### Phase 2: Core TSU Backend (Week 2-3)

#### Step 2.1: Define TSU Interface

```python
# mltsu/tsu_core/interfaces.py
from typing import Protocol, Dict, Optional, Any, Tuple
import numpy as np
from dataclasses import dataclass

@dataclass
class TSUConfig:
    """Configuration for TSU backend"""
    beta: float = 1.0  # Inverse temperature
    num_steps: int = 100  # Sampling steps
    seed: Optional[int] = None
    device: str = "cpu"  # or "cuda", "tpu"

class TSUBackend(Protocol):
    """Protocol for all TSU implementations"""

    def sample_ising(
        self,
        J: np.ndarray,  # Coupling matrix (N, N)
        h: np.ndarray,  # External field (N,)
        beta: float,
        num_steps: int,
        batch_size: int = 1,
        init_state: Optional[np.ndarray] = None,
        record_trajectory: bool = False,
        key: Optional[Any] = None
    ) -> Dict[str, np.ndarray]:
        """Sample from Ising model P(s) âˆ exp(-Î² * E(s))"""
        ...

    def sample_binary_layer(
        self,
        logits: np.ndarray,  # (batch, n_bits)
        beta: float = 1.0,
        num_steps: int = 1,
        key: Optional[Any] = None
    ) -> np.ndarray:
        """Sample binary states from logits"""
        ...

    def sample_custom(
        self,
        energy_fn: callable,
        init_state: np.ndarray,
        num_steps: int,
        beta: float = 1.0,
        key: Optional[Any] = None,
        **kwargs
    ) -> np.ndarray:
        """Generic energy-based sampling"""
        ...
```

#### Step 2.2: Implement JAX Simulator

```python
# mltsu/tsu_jax_sim/state.py
import jax
import jax.numpy as jnp
from typing import NamedTuple

class PBitState(NamedTuple):
    """State of p-bit network"""
    spins: jnp.ndarray  # Current spin configuration {-1, +1}
    energy: jnp.ndarray  # Current energy
    key: jax.random.PRNGKey  # Random state

def init_pbit_state(n_bits: int, key: jax.random.PRNGKey) -> PBitState:
    """Initialize random p-bit state"""
    key, subkey = jax.random.split(key)
    spins = jax.random.choice(subkey, jnp.array([-1, 1]), shape=(n_bits,))
    return PBitState(spins=spins, energy=jnp.zeros(()), key=key)
```

```python
# mltsu/tsu_jax_sim/energy_models.py
import jax.numpy as jnp

def ising_energy(spins: jnp.ndarray, J: jnp.ndarray, h: jnp.ndarray) -> jnp.ndarray:
    """Compute Ising energy: E = -0.5 * s^T J s - h^T s"""
    interaction = -0.5 * jnp.dot(spins, jnp.dot(J, spins))
    field = -jnp.dot(h, spins)
    return interaction + field

def binary_layer_energy(states: jnp.ndarray, logits: jnp.ndarray) -> jnp.ndarray:
    """Energy for independent binary units"""
    return -jnp.sum(states * logits)
```

```python
# mltsu/tsu_jax_sim/sampler.py
import jax
import jax.numpy as jnp
from functools import partial

@partial(jax.jit, static_argnames=['num_steps'])
def gibbs_sample_ising(
    J: jnp.ndarray,
    h: jnp.ndarray,
    beta: float,
    num_steps: int,
    key: jax.random.PRNGKey,
    init_state: Optional[jnp.ndarray] = None
) -> jnp.ndarray:
    """Gibbs sampling for Ising model"""
    n_spins = J.shape[0]

    # Initialize
    if init_state is None:
        key, subkey = jax.random.split(key)
        state = jax.random.choice(subkey, jnp.array([-1, 1]), shape=(n_spins,))
    else:
        state = init_state

    def gibbs_step(carry, _):
        state, key = carry
        key, subkey = jax.random.split(key)

        # Random site selection
        site = jax.random.randint(subkey, (), 0, n_spins)

        # Compute local field
        local_field = J[site] @ state + h[site]

        # Compute probability of spin up
        prob_up = jax.nn.sigmoid(2 * beta * local_field)

        # Sample new spin
        key, subkey = jax.random.split(key)
        new_spin = jax.lax.cond(
            jax.random.uniform(subkey) < prob_up,
            lambda _: 1.0,
            lambda _: -1.0,
            None
        )

        # Update state
        state = state.at[site].set(new_spin)
        return (state, key), state

    # Run sampling
    (final_state, _), trajectory = jax.lax.scan(
        gibbs_step, (state, key), jnp.arange(num_steps)
    )

    return final_state
```

```python
# mltsu/tsu_jax_sim/backend.py
import jax
import jax.numpy as jnp
import numpy as np
from typing import Dict, Optional, Any
from ..tsu_core.interfaces import TSUBackend
from .sampler import gibbs_sample_ising

class JAXTSUBackend:
    """JAX-based TSU simulator"""

    def __init__(self, seed: Optional[int] = None):
        self.key = jax.random.PRNGKey(seed or 0)

    def sample_ising(
        self,
        J: np.ndarray,
        h: np.ndarray,
        beta: float,
        num_steps: int,
        batch_size: int = 1,
        init_state: Optional[np.ndarray] = None,
        record_trajectory: bool = False,
        key: Optional[Any] = None
    ) -> Dict[str, np.ndarray]:
        """Sample from Ising model"""
        # Convert to JAX arrays
        J_jax = jnp.array(J)
        h_jax = jnp.array(h)

        # Get key
        if key is None:
            self.key, key = jax.random.split(self.key)

        # Sample batch
        samples = []
        for _ in range(batch_size):
            key, subkey = jax.random.split(key)
            sample = gibbs_sample_ising(
                J_jax, h_jax, beta, num_steps, subkey, init_state
            )
            samples.append(np.array(sample))

        return {
            'samples': np.stack(samples),
            'final_energy': np.array([self._compute_energy(s, J, h) for s in samples])
        }

    def sample_binary_layer(
        self,
        logits: np.ndarray,
        beta: float = 1.0,
        num_steps: int = 1,
        key: Optional[Any] = None
    ) -> np.ndarray:
        """Sample binary states from logits"""
        logits_jax = jnp.array(logits)

        if key is None:
            self.key, key = jax.random.split(self.key)

        # Simple Bernoulli sampling
        probs = jax.nn.sigmoid(beta * logits_jax)
        key, subkey = jax.random.split(key)
        samples = jax.random.bernoulli(subkey, probs)

        return np.array(samples.astype(jnp.float32))

    def _compute_energy(self, state, J, h):
        """Compute Ising energy"""
        return -0.5 * np.dot(state, np.dot(J, state)) - np.dot(h, state)
```

### Phase 3: PyTorch Integration (Week 3-4)

#### Step 3.1: PyTorch-JAX Bridge

```python
# mltsu/tsu_pytorch/bridge.py
import torch
import numpy as np
from typing import Optional, Union
import jax.dlpack

def torch_to_jax(tensor: torch.Tensor):
    """Convert PyTorch tensor to JAX array via DLPack"""
    return jax.dlpack.from_dlpack(torch.to_dlpack(tensor))

def jax_to_torch(array, device: Optional[torch.device] = None):
    """Convert JAX array to PyTorch tensor via DLPack"""
    dlpack = jax.dlpack.to_dlpack(array)
    tensor = torch.from_dlpack(dlpack)
    if device is not None:
        tensor = tensor.to(device)
    return tensor

def numpy_to_torch(array: np.ndarray, device: Optional[torch.device] = None):
    """Convert numpy array to PyTorch tensor"""
    tensor = torch.from_numpy(array).float()
    if device is not None:
        tensor = tensor.to(device)
    return tensor
```

#### Step 3.2: TSU Binary Layer

```python
# mltsu/tsu_pytorch/binary_layer.py
import torch
import torch.nn as nn
import numpy as np
from typing import Optional
from ..tsu_core.interfaces import TSUBackend
from .bridge import numpy_to_torch

class StraightThroughEstimator(torch.autograd.Function):
    """STE for binary sampling"""

    @staticmethod
    def forward(ctx, input, samples):
        ctx.save_for_backward(input)
        return samples

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        # Pass gradient straight through
        return grad_output, None

class TSUBinaryLayer(nn.Module):
    """Binary sampling layer using TSU backend"""

    def __init__(
        self,
        tsu_backend: TSUBackend,
        beta: float = 1.0,
        num_steps: int = 1,
        use_ste: bool = True
    ):
        super().__init__()
        self.tsu_backend = tsu_backend
        self.beta = beta
        self.num_steps = num_steps
        self.use_ste = use_ste

    def forward(self, logits: torch.Tensor) -> torch.Tensor:
        """
        Forward pass: sample binary states from logits
        Args:
            logits: (batch, n_bits) tensor of logits
        Returns:
            samples: (batch, n_bits) binary tensor {0, 1}
        """
        device = logits.device
        logits_np = logits.detach().cpu().numpy()

        # Sample using TSU backend
        samples_np = self.tsu_backend.sample_binary_layer(
            logits_np, self.beta, self.num_steps
        )

        # Convert back to torch
        samples = numpy_to_torch(samples_np, device)

        # Apply straight-through estimator if training
        if self.training and self.use_ste:
            samples = StraightThroughEstimator.apply(logits, samples)

        return samples
```

#### Step 3.3: TSU Gaussian Noise

```python
# mltsu/tsu_pytorch/noise.py
import torch
import numpy as np
from typing import Tuple, Optional
from ..tsu_core.interfaces import TSUBackend
from .bridge import numpy_to_torch

class TSUGaussianNoise:
    """Generate approximate Gaussian noise using p-bits"""

    def __init__(
        self,
        tsu_backend: TSUBackend,
        M: int = 12,  # p-bits per scalar
        beta: float = 1.0,
        num_steps: int = 1
    ):
        self.tsu_backend = tsu_backend
        self.M = M
        self.beta = beta
        self.num_steps = num_steps

    def sample(
        self,
        shape: Tuple[int, ...],
        device: Optional[torch.device] = None
    ) -> torch.Tensor:
        """
        Sample approximate Gaussian noise N(0,1)
        Uses CLT: sum of M p-bits -> Gaussian
        """
        total_scalars = np.prod(shape)
        total_pbits = total_scalars * self.M

        # Sample binary states
        logits = np.zeros(total_pbits)  # Unbiased p-bits
        samples = self.tsu_backend.sample_binary_layer(
            logits.reshape(1, -1), self.beta, self.num_steps
        ).squeeze()

        # Map {0,1} -> {-1,+1}
        spins = 2 * samples - 1

        # Reshape and sum M p-bits per scalar
        spins = spins.reshape(total_scalars, self.M)
        gaussian = np.sum(spins, axis=1) / np.sqrt(self.M)

        # Reshape to target shape
        gaussian = gaussian.reshape(shape)

        return numpy_to_torch(gaussian, device)
```

#### Step 3.4: Thermodynamic Attention

```python
# mltsu/tsu_pytorch/attention.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Optional
from ..tsu_core.interfaces import TSUBackend
from .bridge import numpy_to_torch

class ThermodynamicAttention(nn.Module):
    """Attention using TSU-sampled patterns instead of softmax"""

    def __init__(
        self,
        d_model: int,
        n_heads: int,
        tsu_backend: TSUBackend,
        n_samples: int = 32,
        beta: float = 1.0,
        dropout: float = 0.1
    ):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        self.n_samples = n_samples
        self.beta = beta
        self.tsu_backend = tsu_backend

        # Standard attention projections
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(
        self,
        x: torch.Tensor,
        mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Thermodynamic attention forward pass
        Args:
            x: (batch, seq_len, d_model)
            mask: (batch, seq_len, seq_len) attention mask
        """
        batch_size, seq_len, _ = x.shape
        device = x.device

        # Compute Q, K, V
        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k)
        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k)
        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k)

        # Transpose for attention computation
        Q = Q.transpose(1, 2)  # (batch, heads, seq, d_k)
        K = K.transpose(1, 2)
        V = V.transpose(1, 2)

        # Compute attention scores (energies)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)

        # Convert scores to energies (negative for TSU)
        energies = -scores

        if mask is not None:
            energies = energies.masked_fill(mask == 0, float('inf'))

        # Sample attention patterns using TSU
        attn_weights = self._sample_attention(energies)

        # Apply attention
        attn_output = torch.matmul(attn_weights, V)

        # Reshape and project
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, seq_len, self.d_model)
        output = self.W_o(attn_output)

        return self.dropout(output)

    def _sample_attention(self, energies: torch.Tensor) -> torch.Tensor:
        """
        Sample attention weights using TSU
        Args:
            energies: (batch, heads, seq, seq) attention energies
        Returns:
            weights: (batch, heads, seq, seq) sampled attention weights
        """
        batch, heads, seq_q, seq_k = energies.shape
        device = energies.device

        # Flatten for TSU sampling
        energies_flat = energies.reshape(-1, seq_k)
        energies_np = energies_flat.detach().cpu().numpy()

        # Sample binary attention patterns
        all_samples = []
        for i in range(energies_np.shape[0]):
            query_samples = []
            for _ in range(self.n_samples):
                # Sample which keys this query attends to
                logits = -energies_np[i] * self.beta  # Convert energy to logits
                sample = self.tsu_backend.sample_binary_layer(
                    logits.reshape(1, -1), beta=1.0, num_steps=1
                )
                query_samples.append(sample.squeeze())

            # Average samples to get attention probabilities
            avg_attention = np.mean(query_samples, axis=0)

            # Normalize to sum to 1
            if avg_attention.sum() > 0:
                avg_attention = avg_attention / avg_attention.sum()
            else:
                avg_attention = np.ones(seq_k) / seq_k

            all_samples.append(avg_attention)

        # Convert back to torch
        weights_np = np.stack(all_samples)
        weights = numpy_to_torch(weights_np, device)
        weights = weights.reshape(batch, heads, seq_q, seq_k)

        return weights
```

### Phase 4: Streamlit Applications (Week 4-5)

#### Step 4.1: Ising Playground

```python
# mltsu/streamlit/ising_app.py
import streamlit as st
import numpy as np
import plotly.graph_objects as go
from mltsu.tsu_jax_sim.backend import JAXTSUBackend
import time

st.set_page_config(page_title="TSU Ising Playground", layout="wide")
st.title("ðŸ”¬ Thermodynamic Ising Model Playground")

# Sidebar controls
st.sidebar.header("Ising Model Parameters")
n_spins = st.sidebar.slider("Number of spins", 4, 20, 10)
coupling_strength = st.sidebar.slider("Coupling strength (J)", -2.0, 2.0, 1.0)
field_strength = st.sidebar.slider("External field (h)", -2.0, 2.0, 0.0)
beta = st.sidebar.slider("Inverse temperature (Î²)", 0.1, 10.0, 1.0)
num_steps = st.sidebar.slider("Sampling steps", 10, 1000, 100)

# Initialize backend
@st.cache_resource
def get_backend():
    return JAXTSUBackend(seed=42)

backend = get_backend()

# Create Ising model
J = np.random.randn(n_spins, n_spins) * coupling_strength
J = (J + J.T) / 2  # Symmetrize
np.fill_diagonal(J, 0)  # No self-interaction
h = np.ones(n_spins) * field_strength

# Sample button
col1, col2 = st.columns(2)

with col1:
    st.subheader("Coupling Matrix J")
    fig_j = go.Figure(data=go.Heatmap(z=J, colorscale='RdBu'))
    fig_j.update_layout(height=400)
    st.plotly_chart(fig_j, use_container_width=True)

with col2:
    st.subheader("Sampling Dynamics")

    if st.button("Run Sampling"):
        progress_bar = st.progress(0)

        # Sample with trajectory
        samples = []
        energies = []

        for i in range(10):
            result = backend.sample_ising(
                J, h, beta, num_steps // 10, batch_size=1
            )
            sample = result['samples'][0]
            energy = result['final_energy'][0]

            samples.append(sample)
            energies.append(energy)
            progress_bar.progress((i + 1) / 10)

        # Plot results
        st.subheader("Energy Evolution")
        fig_energy = go.Figure()
        fig_energy.add_trace(go.Scatter(
            y=energies,
            mode='lines+markers',
            name='Energy'
        ))
        fig_energy.update_layout(
            xaxis_title="Sample",
            yaxis_title="Energy",
            height=300
        )
        st.plotly_chart(fig_energy, use_container_width=True)

        # Show final state
        st.subheader("Final Spin Configuration")
        final_state = samples[-1].reshape(int(np.sqrt(n_spins)), -1)
        fig_state = go.Figure(data=go.Heatmap(
            z=final_state,
            colorscale='RdBu',
            zmid=0
        ))
        fig_state.update_layout(height=400)
        st.plotly_chart(fig_state, use_container_width=True)

# Statistics
st.sidebar.header("Sampling Statistics")
st.sidebar.info(f"""
**Model Info:**
- Spins: {n_spins}
- Parameters: {n_spins * (n_spins - 1) // 2 + n_spins}
- Temperature: {1/beta:.2f}
""")
```

### Phase 5: Testing Framework (Week 5)

#### Step 5.1: Core Tests

```python
# tests/test_core.py
import pytest
import numpy as np
import torch
from mltsu.tsu_jax_sim.backend import JAXTSUBackend
from mltsu.tsu_pytorch.binary_layer import TSUBinaryLayer
from mltsu.tsu_pytorch.noise import TSUGaussianNoise

class TestTSUBackend:
    def setup_method(self):
        self.backend = JAXTSUBackend(seed=42)

    def test_ising_sampling(self):
        """Test Ising model sampling"""
        n_spins = 10
        J = np.random.randn(n_spins, n_spins)
        J = (J + J.T) / 2
        h = np.random.randn(n_spins)

        result = self.backend.sample_ising(
            J, h, beta=1.0, num_steps=100, batch_size=5
        )

        assert result['samples'].shape == (5, n_spins)
        assert result['final_energy'].shape == (5,)
        assert np.all(np.abs(result['samples']) == 1)  # Spins are Â±1

    def test_binary_layer_sampling(self):
        """Test binary layer sampling"""
        batch_size, n_bits = 8, 16
        logits = np.random.randn(batch_size, n_bits)

        samples = self.backend.sample_binary_layer(logits, beta=1.0)

        assert samples.shape == (batch_size, n_bits)
        assert np.all((samples == 0) | (samples == 1))

class TestPyTorchIntegration:
    def setup_method(self):
        self.backend = JAXTSUBackend(seed=42)

    def test_binary_layer_forward(self):
        """Test TSUBinaryLayer forward pass"""
        layer = TSUBinaryLayer(self.backend, beta=1.0)
        logits = torch.randn(4, 8)

        samples = layer(logits)

        assert samples.shape == logits.shape
        assert torch.all((samples == 0) | (samples == 1))

    def test_binary_layer_gradient(self):
        """Test gradient flow through TSUBinaryLayer"""
        layer = TSUBinaryLayer(self.backend, beta=1.0, use_ste=True)
        logits = torch.randn(4, 8, requires_grad=True)

        samples = layer(logits)
        loss = samples.sum()
        loss.backward()

        assert logits.grad is not None
        assert logits.grad.shape == logits.shape

    def test_gaussian_noise(self):
        """Test Gaussian noise generation"""
        noise_gen = TSUGaussianNoise(self.backend, M=12)

        samples = noise_gen.sample((100, 10))

        assert samples.shape == (100, 10)
        # Check approximate normality
        assert abs(samples.mean().item()) < 0.2
        assert abs(samples.std().item() - 1.0) < 0.2

if __name__ == "__main__":
    pytest.main([__file__])
```

### Phase 6: Example Notebooks (Week 5-6)

#### Step 6.1: Basic Usage Example

```python
# examples/basic_usage.py
"""
Basic MLTSU Usage Example
Shows how to use TSU components in PyTorch models
"""

import torch
import torch.nn as nn
from mltsu.tsu_jax_sim.backend import JAXTSUBackend
from mltsu.tsu_pytorch.binary_layer import TSUBinaryLayer
from mltsu.tsu_pytorch.noise import TSUGaussianNoise
from mltsu.tsu_pytorch.attention import ThermodynamicAttention

# Initialize TSU backend
backend = JAXTSUBackend(seed=42)

# Example 1: Binary Sampling Layer
print("Example 1: Binary Sampling")
binary_layer = TSUBinaryLayer(backend, beta=2.0)
logits = torch.randn(8, 16)  # 8 samples, 16 bits each
binary_samples = binary_layer(logits)
print(f"Input logits shape: {logits.shape}")
print(f"Binary samples shape: {binary_samples.shape}")
print(f"Sample values: {binary_samples[0][:8]}")

# Example 2: Gaussian Noise
print("\nExample 2: TSU Gaussian Noise")
noise_gen = TSUGaussianNoise(backend, M=12)
noise = noise_gen.sample((4, 32, 32))  # 4 images, 32x32
print(f"Noise shape: {noise.shape}")
print(f"Noise stats - Mean: {noise.mean():.3f}, Std: {noise.std():.3f}")

# Example 3: Thermodynamic Attention
print("\nExample 3: Thermodynamic Attention")
d_model, n_heads = 128, 8
attn = ThermodynamicAttention(d_model, n_heads, backend, n_samples=16)
x = torch.randn(2, 10, d_model)  # batch=2, seq_len=10
output = attn(x)
print(f"Input shape: {x.shape}")
print(f"Output shape: {output.shape}")

# Example 4: Simple Model with TSU Components
class TSUModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, tsu_backend):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.binary = TSUBinaryLayer(tsu_backend, beta=1.0)
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        mask = self.binary(x)  # Binary dropout mask
        x = x * mask
        return self.fc2(x)

print("\nExample 4: TSU Model")
model = TSUModel(64, 128, 10, backend)
x = torch.randn(32, 64)
output = model(x)
print(f"Model output shape: {output.shape}")
```

## ðŸ”¬ Scientific Validation

### Benchmarks to Run

1. **Sampling Convergence**
   - Compare TSU vs standard samplers
   - Measure samples needed for convergence
   - Energy landscape exploration

2. **Statistical Properties**
   - Calibration analysis
   - Entropy measurements
   - Distribution matching tests

3. **Performance Metrics**
   - Sampling throughput
   - Memory usage
   - Energy estimates (simulated)

### Key Experiments

1. **Ising Ground State Finding**
   - Test on known Ising problems
   - Compare with exact solutions
   - Benchmark against classical optimizers

2. **Attention Pattern Analysis**
   - Compare thermodynamic vs softmax attention
   - Measure sparsity patterns
   - Analyze interpretability

3. **Diffusion Model Quality**
   - FID scores with TSU noise
   - Sample diversity metrics
   - Training stability

## ðŸ“š Documentation

### API Documentation Structure

```markdown
# MLTSU API Reference

## Core Interfaces
- TSUBackend: Protocol for all backends
- TSUConfig: Configuration dataclass

## JAX Simulator
- JAXTSUBackend: Reference implementation
- Sampling algorithms: Gibbs, Langevin

## PyTorch Components
- TSUBinaryLayer: Binary sampling layer
- TSUGaussianNoise: Approximate Gaussian noise
- ThermodynamicAttention: TSU-based attention

## Models
- TinyThermoLM: Demo language model
- TSUDiffusion: Diffusion with TSU noise
```

## ðŸš€ Deployment & Next Steps

### Making it Production-Ready

1. **Package for PyPI**
```bash
python setup.py sdist bdist_wheel
twine upload dist/*
```

2. **Docker Container**
```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["streamlit", "run", "mltsu/streamlit/ising_app.py"]
```

3. **Hardware Integration Prep**
- Abstract hardware communication layer
- Define hardware capability discovery
- Create fallback mechanisms
- Build performance monitoring

### Future Extensions

1. **Advanced Samplers**
   - Parallel tempering
   - Replica exchange
   - Quantum-inspired algorithms

2. **Model Zoo**
   - Pre-trained thermodynamic models
   - Benchmark suites
   - Reference implementations

3. **Hardware Backends**
   - Extropic TSU integration
   - FPGA accelerators
   - Custom ASIC support

## ðŸŽ¯ Success Criteria

- [ ] JAX simulator achieves correct Boltzmann distributions
- [ ] PyTorch integration maintains gradient flow
- [ ] Thermodynamic attention matches/exceeds softmax quality
- [ ] Streamlit apps provide intuitive visualization
- [ ] Tests achieve >90% coverage
- [ ] Documentation is complete and clear
- [ ] Benchmarks show sampling advantages
- [ ] Code is hardware-backend agnostic

## ðŸ“ž Support & Community

- GitHub: github.com/[your-org]/mltsu
- Documentation: mltsu.readthedocs.io
- Discord: [Community Server]
- Email: mltsu@[your-domain].com

---

**Remember**: This is about building the bridge, not claiming magical speedups. Focus on clean interfaces, scientific rigor, and preparing for real thermodynamic hardware when it arrives.