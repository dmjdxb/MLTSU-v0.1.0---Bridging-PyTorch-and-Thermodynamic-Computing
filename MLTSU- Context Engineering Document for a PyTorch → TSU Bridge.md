MLTSU: Context Engineering Document for a PyTorch â†’ TSU Bridge

Thermodynamic Sampling for Large Language Models

0. Preamble

This document defines the conceptual, mathematical, and software architecture for MLTSU: a PyTorch-first stack where:

GPUs/CPUs handle deterministic linear algebra (matmuls, layer norms, etc.), and

Thermodynamic Sampling Units (TSUs) / p-bit hardware handle probabilistic sampling (attention patterns, noise, negative examples, memory retrieval, etc.).

The endgame: create the canonical bridge from mainstream deep learning (PyTorch) to emerging thermodynamic hardware (Extropic-style TSUs), starting with simulators and becoming hardware-ready the moment real TSU devices are available.

This is not â€œwe magically train GPT-4 at 1000Ã— lower wall-plug energy.â€ It is:

â€œWe design the APIs, model patterns, and reference implementations that let thermodynamic hardware actually slot into modern LLM training and inference.â€

1. Modern history of probabilistic computing & p-bits
1.1 From Ising models to Ising machines

The Ising model began as a toy for ferromagnets: a set of spins 
ğ‘ 
ğ‘–
âˆˆ
{
âˆ’
1
,
+
1
}
s
i
	â€‹

âˆˆ{âˆ’1,+1} with energy

ğ¸
(
ğ‘ 
)
=
âˆ’
âˆ‘
ğ‘–
<
ğ‘—
ğ½
ğ‘–
ğ‘—
ğ‘ 
ğ‘–
ğ‘ 
ğ‘—
âˆ’
âˆ‘
ğ‘–
â„
ğ‘–
ğ‘ 
ğ‘–
,
E(s)=âˆ’
i<j
âˆ‘
	â€‹

J
ij
	â€‹

s
i
	â€‹

s
j
	â€‹

âˆ’
i
âˆ‘
	â€‹

h
i
	â€‹

s
i
	â€‹

,

whose low-energy configurations correspond to ordered phases. Over time, it became the Swiss army knife of combinatorial optimization: many NP-hard problems (Max-Cut, SAT, TSP) can be mapped into Ising Hamiltonians whose ground states encode optimal solutions.

Because of that, an entire ecosystem of Ising machines emerged:

Coherent Ising machines (optical/quantum-inspired).

Hardware Boltzmann / Ising machines in CMOS and spintronics.

â€œProbabilistic Ising Machinesâ€ (PIMs), which explicitly embrace stochastic dynamics to sample from Boltzmann distributions.

These machines effectively outsource sampling from complicated distributions to physics itself, instead of simulating everything numerically.

1.2 Probabilistic bits (p-bits)

Classical bits are stable: 0 or 1 until flipped. Quantum bits (qubits) are coherent superpositions, with all the fun and pain that entails.

Probabilistic bits, or p-bits, sit in between: they are classical entities that fluctuate randomly between 0 and 1, with a tunable bias. A simple model:

ğ‘š
(
ğ‘¡
)
âˆˆ
{
âˆ’
1
,
+
1
}
,
ğ‘ƒ
(
ğ‘š
(
ğ‘¡
)
=
+
1
)
=
ğœ
(
ğ›½
ğ¼
)
,
m(t)âˆˆ{âˆ’1,+1},P(m(t)=+1)=Ïƒ(Î²I),

where 
ğ¼
I is an effective â€œinput currentâ€ and 
ğœ
Ïƒ is a logistic function.

Key ideas from Camsari, Datta and others:

p-bits can be realized in low-barrier nanomagnets, CMOS circuits, or other noisy devices.

Networks of coupled p-bits can implement Boltzmann machines, logical circuits, and optimization solvers.

Because they run at room temperature and leverage natural fluctuations, they are good candidates for probabilistic computers (p-computers).

Recent work shows p-bit based probabilistic Ising machines can be built in integrated CMOS+MTJ platforms, with nanosecond update times and milliwatt-scale power budgets.

1.3 Probabilistic computing as â€œp-computersâ€

The emerging narrative:

p-computers are architectures where the primitive is sampling from a distribution, not evaluating a deterministic logic function.

They are well-suited to:

Combinatorial optimization (finding low-energy states).

Probabilistic inference (Boltzmann machines, Markov Random Fields).

Stochastic machine learning tasks (e.g. generative models, energy-based models).

This is exactly the regime where LLMs also spend a lot of time: sampling from high-dimensional distributions, either during training (noise, negatives) or inference (token generation).

2. Thermodynamic computing & TSUs
2.1 Extropic-style thermodynamic sampling units

Extropic is pushing a specific vision: Thermodynamic Sampling Units (TSUs) â€“ chips that exploit thermal noise in transistor networks to sample from parameterized probability distributions.

Roughly:

You encode an energy-based model (EBM) or related structure into a TSUâ€™s parameters (weights, couplings, biases).

The TSU physically relaxes toward low-energy states, emitting samples from the corresponding distribution.

Extropic claims ~10,000Ã— energy savings on certain Denoising Thermodynamic Model (DTM) generative benchmarks compared with GPU baselines.

Important caveats:

These gains are currently simulation- and benchmark-specific, not a universal constant.

TSUs excel at sampling tasks, not dense linear algebra per se.

This aligns almost perfectly with p-bit / Ising-machine literature: use physics as a Bayesian sampler, not as a deterministic matmul engine.

2.2 Why LLMs care about sampling

LLM training/inference involves:

Deterministic modules:

Matmuls (QKV projections, MLPs),

Layernorm, residual adds.

Stochastic or combinatorial modules:

Attention pattern selection (implicitly via softmax),

Dropout / stochastic depth,

Negative sampling (contrastive / EBM / alignment),

Token sampling at inference,

Routing (Mixture-of-Experts, sparsity patterns),

Retrieval and memory access decisions.

TSUs map naturally to the second category. Thatâ€™s where MLTSU lives.

3. Design goals for MLTSU
3.1 High-level goals

MLTSU is a PyTorch-native framework that:

Treats TSUs as a sampling co-processor:

PyTorch keeps doing forward/backward passes and matmuls.

TSUs generate stochastic and combinatorial structures.

Exposes a clean, minimal interface that:

Runs today with a JAX-based p-bit simulator,

Can be swapped for real TSU hardware later without changing model code.

Provides reference model patterns:

Thermodynamic attention layers.

TSU-backed Gaussian noise.

TSU-based hard negative sampling for EBM/contrastive losses.

(Later) TSU-based memory/retrieval modules.

Is scientifically respectable:

Tied to Ising/p-bit literature and EBM theory.

Designed for experiments comparing TSU-based vs conventional sampling.

3.2 Non-goals (for now)

Not replacing all matmuls with analog/Ising operations.

Not replacing backpropagation with fully thermodynamic gradient descent.

Not promising end-to-end â€œ1000Ã— cheaper GPT-4 trainingâ€ today.

4. Core architectural idea
4.1 Two-plane architecture

Think of MLTSU as two planes:

Deterministic plane (GPU/CPU + PyTorch)

Handles all continuous differentiable computation:

Embeddings

Linear layers / MLPs

LayerNorm, residuals

Loss computation (CE, MSE, etc.)

Backprop is standard PyTorch autograd.

Probabilistic plane (TSU / p-bit backend)

Handles discrete or stochastic steps via sampling:

Binary masks (attention, dropout, sparsity).

Approximate Gaussian noise (via CLT on p-bits).

Discrete choices (token candidates, routing).

Energy-based negative sampling.

These planes talk through a well-defined interface (TSUBackend). The core conceptual contract:

Give TSU a description of an energy landscape or logits â†’ get back samples from the corresponding distribution.

4.2 The TSUBackend interface

In code terms (conceptually):

class TSUBackend(Protocol):
    def sample_ising(J, h, beta, num_steps, batch_size, init_state, record_trajectory, key) -> dict:
        """
        Sample from an Ising model with couplings J and fields h.
        """

    def sample_binary_layer(logits, beta, num_steps, key) -> Array:
        """
        Given input logits (for each bit), return samples in {0,1} or {-1,+1}.
        """

    def sample_custom(energy_fn, init_state, num_steps, beta, key, **kwargs) -> Array:
        """
        Generic energy-based sampling hook.
        """


Different backends implement this:

JAXTSUBackend â€“ JAX-based p-bit simulator (today).

ExtropicTSUBackend â€“ real TSU hardware (future).

Others â€“ e.g. software Gibbs samplers, FPGA prototypes, etc.

4.3 PyTorch integration pattern

PyTorch modules never talk directly to JAX, PCIe, or hardware. They only depend on TSUBackend. Example patterns:

TSU binary sampling layer

class TSUBinaryLayer(nn.Module):
    def __init__(self, tsu_backend, beta=1.0, num_steps=1):
        ...

    def forward(self, logits: torch.Tensor) -> torch.Tensor:
        """
        Forward: use TSU to sample binary states.
        Backward: use straight-through estimator so gradients flow through logits.
        """


TSU Gaussian noise generator

class TSUGaussianNoise:
    def __init__(self, tsu_backend, M=12, beta=1.0, num_steps=1):
        ...

    def sample(self, shape, device) -> torch.Tensor:
        """
        Use M p-bits per scalar, map {0,1}-> {-1,+1}, sum and normalize to approximate N(0,1).
        """


Thermodynamic attention layer

class ThermodynamicAttention(nn.Module):
    def __init__(self, d_model, n_heads, tsu_backend, n_samples=32, beta=1.0):
        ...

    def forward(self, x, mask=None) -> torch.Tensor:
        """
        Q,K,V as usual, but attention weights are sampled via TSU-backed binary patterns
        instead of softmax.
        """


TSU negative sampler

class TSUNegativeSampler(nn.Module):
    def __init__(self, tsu_backend, n_negatives: int):
        ...

    def forward(self, energy: torch.Tensor, target_onehot: torch.Tensor):
        """
        Use TSU to sample â€œhardâ€ negative tokens from low-energy competitors.
        """


This modularity is the core of the â€œbridge.â€

5. Mathematical grounding for TSU primitives
5.1 p-bits as noisy logistic units

A typical idealized p-bit follows something like:

ğ‘š
ğ‘–
(
ğ‘¡
)
âˆˆ
{
âˆ’
1
,
+
1
}
,
ğ‘ƒ
(
ğ‘š
ğ‘–
(
ğ‘¡
)
=
+
1
âˆ£
ğ¼
ğ‘–
)
=
ğœ
(
ğ›½
ğ¼
ğ‘–
)
,
m
i
	â€‹

(t)âˆˆ{âˆ’1,+1},P(m
i
	â€‹

(t)=+1
	â€‹

I
i
	â€‹

)=Ïƒ(Î²I
i
	â€‹

),

with 
ğ¼
ğ‘–
=
âˆ‘
ğ‘—
ğ‘Š
ğ‘–
ğ‘—
ğ‘š
ğ‘—
+
ğ‘
ğ‘–
I
i
	â€‹

=âˆ‘
j
	â€‹

W
ij
	â€‹

m
j
	â€‹

+b
i
	â€‹

. Under asynchronous updating and symmetric weights, such a network converges to a Boltzmann distribution

ğ‘ƒ
(
ğ‘š
)
âˆ
ğ‘’
âˆ’
ğ›½
ğ¸
(
ğ‘š
)
,
ğ¸
(
ğ‘š
)
=
âˆ’
1
2
âˆ‘
ğ‘–
,
ğ‘—
ğ‘Š
ğ‘–
ğ‘—
ğ‘š
ğ‘–
ğ‘š
ğ‘—
âˆ’
âˆ‘
ğ‘–
ğ‘
ğ‘–
ğ‘š
ğ‘–
.
P(m)âˆe
âˆ’Î²E(m)
,E(m)=âˆ’
2
1
	â€‹

i,j
âˆ‘
	â€‹

W
ij
	â€‹

m
i
	â€‹

m
j
	â€‹

âˆ’
i
âˆ‘
	â€‹

b
i
	â€‹

m
i
	â€‹

.

Our TSU backend abstracts away how this sampling is implemented (spintronics, thermodynamic silicon, JAX simulation) and exposes only the resulting sampling oracle.

5.2 TSU binary layer

Given logits 
â„“
ğ‘–
â„“
i
	â€‹

, you can interpret them as:

Either direct inputs 
ğ¼
ğ‘–
=
â„“
ğ‘–
I
i
	â€‹

=â„“
i
	â€‹

 with

ğ‘ƒ
(
ğ‘¥
ğ‘–
=
1
)
=
ğœ
(
ğ›½
â„“
ğ‘–
)
,
P(x
i
	â€‹

=1)=Ïƒ(Î²â„“
i
	â€‹

),

Or parameters of an energy function 
ğ¸
(
ğ‘¥
)
E(x) where 
ğ‘¥
ğ‘–
âˆˆ
{
0
,
1
}
x
i
	â€‹

âˆˆ{0,1} and

ğ‘ƒ
(
ğ‘¥
)
âˆ
exp
â¡
(
âˆ’
ğ›½
ğ¸
(
ğ‘¥
)
)
.
P(x)âˆexp(âˆ’Î²E(x)).

The TSU binary layer gives you samples 
ğ‘¥
x according to such distributions. Thatâ€™s enough for:

Stochastic attention masks: sample which keys a given query attends to.

Dropout masks: drop neurons according to thermodynamic distribution.

Sparsity patterns: sample which weights/neurons are â€œon.â€

5.3 TSU Gaussian via central limit theorem

To get approximate Gaussian noise from p-bits:

For each scalar we want, sample 
ğ‘€
M p-bits 
ğ‘
ğ‘—
âˆˆ
{
0
,
1
}
b
j
	â€‹

âˆˆ{0,1}.

Map to spins 
ğ‘ 
ğ‘—
=
2
ğ‘
ğ‘—
âˆ’
1
âˆˆ
{
âˆ’
1
,
+
1
}
s
j
	â€‹

=2b
j
	â€‹

âˆ’1âˆˆ{âˆ’1,+1}.

Compute

ğ‘§
=
1
ğ‘€
âˆ‘
ğ‘—
=
1
ğ‘€
ğ‘ 
ğ‘—
.
z=
M
	â€‹

1
	â€‹

j=1
âˆ‘
M
	â€‹

s
j
	â€‹

.

By the central limit theorem, as 
ğ‘€
M grows, 
ğ‘§
â‰ˆ
ğ‘
(
0
,
1
)
zâ‰ˆN(0,1) if the p-bits are weakly correlated. This is enough to replace torch.randn_like with TSU-backed noise in:

Diffusion models.

Bayesian weight sampling.

Gradient noise injection.

5.4 Thermodynamic attention

Standard attention:

Attn
(
ğ‘„
,
ğ¾
,
ğ‘‰
)
=
softmax
(
ğ‘„
ğ¾
âŠ¤
ğ‘‘
ğ‘˜
)
ğ‘‰
.
Attn(Q,K,V)=softmax(
d
k
	â€‹

	â€‹

QK
âŠ¤
	â€‹

)V.

We reinterpret this as:

Scores: 
ğ‘†
=
ğ‘„
ğ¾
âŠ¤
ğ‘‘
ğ‘˜
S=
d
k
	â€‹

	â€‹

QK
âŠ¤
	â€‹

 (higher = more preferred).

Energy: 
ğ¸
=
âˆ’
ğ‘†
E=âˆ’S (lower = more preferred).

Instead of computing a softmax distribution, we sample binary attention patterns 
ğ‘
âˆˆ
{
0
,
1
}
ğ‘‡
aâˆˆ{0,1}
T
 for each query using TSU:

ğ‘ƒ
(
ğ‘
âˆ£
ğ¸
)
âˆ
exp
â¡
(
âˆ’
ğ›½
â€‰
ğ‘
âŠ¤
ğ¸
)
,
P(aâˆ£E)âˆexp(âˆ’Î²a
âŠ¤
E),

then approximate attention weights via Monte Carlo:

ğ‘¤
^
=
1
ğ‘
âˆ‘
ğ‘›
=
1
ğ‘
ğ‘
(
ğ‘›
)
.
w
^
=
N
1
	â€‹

n=1
âˆ‘
N
	â€‹

a
(n)
.

These approximate probabilities of attending to each key. You can then:

Normalize 
ğ‘¤
^
w
^
 to sum to 1.

Compute output as 
ğ‘¤
^
ğ‘‰
w
^
V.

Gradients flow through the pre-sampling logits using STE.

5.5 TSU negative sampling

In energy-based LM objectives, one often wants:

Low energy for the correct token,

High energy for plausible but incorrect tokens (â€œhard negativesâ€).

Given an energy tensor 
ğ¸
âˆˆ
ğ‘…
ğµ
Ã—
ğ‘‡
Ã—
ğ‘‰
EâˆˆR
BÃ—TÃ—V
, rather than:

Sampling negatives uniformly or via top-k on GPU,

we use TSUs to sample low-energy alternatives:

ğ‘ƒ
(
neg token
=
ğ‘£
)
âˆ
exp
â¡
(
âˆ’
ğ›½
ğ¸
ğ‘
,
ğ‘¡
,
ğ‘£
)
,
P(neg token=v)âˆexp(âˆ’Î²E
b,t,v
	â€‹

),

conditioned on excluding the true target. TSUs give us efficient ways to explore the tail of this distribution, which may be energy-costly on a GPU.

6. Software architecture of MLTSU
6.1 Repository layout (conceptual)
mltsu/
  tsu_core/
    interfaces.py        # TSUBackend, common API
    utils.py

  tsu_jax_sim/
    state.py             # p-bit / Ising state (JAX)
    energy_models.py     # Ising, simple EBMs
    sampler.py           # Gibbs / Langevin / binary layer sampling
    backend.py           # JAXTSUBackend(TSUBackend)

  tsu_pytorch/
    bridge.py            # torch <-> JAX interop via DLPack
    binary_layer.py      # TSUBinaryLayer
    noise.py             # TSUGaussianNoise
    attention.py         # ThermodynamicAttention
    negatives.py         # TSUNegativeSampler
    memory.py            # (future) ThermodynamicMemory

  models/
    tiny_thermo_lm.py    # small transformer using thermodynamic attention
    mnist_tsu_diffusion.py

  streamlit/
    ising_app.py         # scientist playground
    attention_viz.py
    lm_playground.py

  docs/
    context_engineering.md  # this document
    math_notes.md           # detailed derivations and experiments

6.2 TSUBackend variants

JAXTSUBackend

Implemented using JAX for fast vectorized Gibbs / Langevin sampling.

Provides sample_ising, sample_binary_layer.

Used for development and reproducible experiments.

Software reference backend

Pure Python/NumPy implementation for debugging or platforms without JAX.

ExtropicTSUBackend (future)

Wraps Extropicâ€™s XTR-0 TSU SDK.

Likely communicates over PCIe, Ethernet or similar.

Same interface, different latency/throughput and, crucially, energy behavior.

7. Model-level design patterns and use cases
7.1 Thermodynamic attention in LLM blocks

A typical transformer block with MLTSU:

class ThermoBlock(nn.Module):
    def __init__(self, d_model, n_heads, tsu_backend):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        self.attn = ThermodynamicAttention(d_model, n_heads, tsu_backend)
        self.ln2 = nn.LayerNorm(d_model)
        self.mlp  = FeedForward(d_model)

    def forward(self, x, mask=None):
        x = x + self.attn(self.ln1(x), mask=mask)
        x = x + self.mlp(self.ln2(x))
        return x


Experiments to run:

Compare standard softmax vs thermodynamic attention in:

Perplexity,

Attention sparsity,

Calibration/uncertainty,

Robustness to noise.

How many TSU samples per head per token are needed to approximate softmax behavior?

7.2 TSU-backed noise in diffusion models

Use TSUGaussianNoise in a DDPM / diffusion model:

Forward process: use TSU noise for 
ğœ–
Ïµ in 
ğ‘¥
ğ‘¡
=
ğ›¼
Ë‰
ğ‘¡
ğ‘¥
0
+
1
âˆ’
ğ›¼
Ë‰
ğ‘¡
ğœ–
x
t
	â€‹

=
Î±
Ë‰
t
	â€‹

	â€‹

x
0
	â€‹

+
1âˆ’
Î±
Ë‰
t
	â€‹

	â€‹

Ïµ.

Reverse process: use TSU noise for the stochastic part of sampling.

Compare with standard Gaussian RNG in:

Sample quality (FID),

Diversity,

Robustness to under/over-fitting.

This is tightly aligned with Extropicâ€™s claimed Denoising Thermodynamic Models (DTM) story.

7.3 TSU negative sampling for energy-based LM objectives

Use TSUNegativeSampler as part of an auxiliary loss:

Compute energies / logits for all vocab items.

Sample TSU-based hard negatives.

Use a margin-based or contrastive energy loss that:

Lowers energy of true targets.

Raises energy of TSU-sampled negatives.

This emphasizes TSUs as probabilistic EBM accelerators for language.

7.4 Thermodynamic memory (future work)

Long-context LLMs struggle with:

Quadratic attention cost.

Deciding which past tokens/segments to attend to.

A thermodynamic memory module could:

Compress segments into energy representations.

Use TSU sampling to perform probabilistic retrieval of relevant segments given a query energy pattern.

Act as a learned, physics-backed retrieval mechanism.

This aligns with p-bit and Ising work on associative memory and pattern retrieval.

8. Evaluation plan & scientific questions

To make this credible to Extropic and the broader community, MLTSU should be evaluated along several axes.

8.1 Functional correctness

Does thermodynamic attention produce sensible attention maps?

Do diffusion models with TSU noise match baseline quality?

Are energy-based LM losses with TSU negatives stable to train?

8.2 Statistical properties

How does TSU sampling affect:

Calibration (expected calibration error),

Uncertainty estimation (entropy of logits, variance across samples),

Sparsity patterns (e.g., how many keys a query attends to)?

Can we shape attention via energy priors more directly than via softmax?

8.3 Algorithmic efficiency (simulator phase)

Even before real hardware, we can ask:

For a fixed budget of samples, does TSU-style sampling converge faster to good structures (negatives, masks) than e.g. Gumbel-softmax, top-k, or dropout-style schemes?

Are there regimes where TSU-based samplers discover â€œharderâ€ negatives that standard heuristics miss?

8.4 Hardware-in-the-loop energy/performance (future)

Once real TSUs are accessible:

Measure energy per:

Sampled attention mask,

Gaussian noise vector,

Hard negative batch,

Memory retrieval query.

Compare:

Energy per operation vs GPU/CPU RNG and sampling.

End-to-end training energy fraction attributable to TSU-accelerated parts.

This is where claim ranges like â€œup to 10,000Ã— per sampling workloadâ€ can be empirically tested in ML contexts.

9. Roadmap
Phase 1 â€“ Core bridge & simulators

Implement TSUBackend and JAXTSUBackend.

Implement TSUBinaryLayer, TSUGaussianNoise.

Build:

Ising playground (Streamlit).

Diffusion demo with TSU noise.

Phase 2 â€“ Thermodynamic attention + tiny LM

Implement ThermodynamicAttention.

Build a small decoder-only LM using this attention.

Train on a modest dataset; compare against softmax baseline.

Phase 3 â€“ TSU negative sampling & EB auxiliary loss

Implement TSUNegativeSampler and SimpleEnergyBasedLMObjective.

Integrate as a side loss in tiny LM.

Measure impact on calibration, robustness, representation quality.

Phase 4 â€“ Documentation & scientist UX

Finalize this context document and a companion math_notes.md.

Ship Streamlit apps for:

Ising dynamics,

Thermodynamic attention visualization,

LM sampling with TSU vs softmax.

Phase 5 â€“ Hardware integration

Implement ExtropicTSUBackend once APIs/SDKs are available.

Run side-by-side experiments:

Same model, same training loop,

Backend = simulator vs real TSU,

Measure latency, throughput, energy, and statistical behavior.

10. Summary

MLTSU is not a marketing line about â€œ1000Ã— cheaper GPT tomorrow.â€ Itâ€™s a concrete stack to:

Bring p-bit / thermodynamic sampling into mainstream PyTorch models,

Align with the physics-driven literature on probabilistic computing and Ising machines,

Provide clean APIs that Extropic-style TSUs can implement,

Demonstrate real, end-to-end models (LLMs, diffusion) where:

Deterministic math runs on GPUs/CPUs,

Stochastic structure is delegated to thermodynamic hardware.

If we get this right, then when TSUs mature, youâ€™re not scrambling to invent a new stackâ€”youâ€™re already holding the reference PyTorch â†’ TSU bridge that people plug new probabilistic hardware into.

Thatâ€™s the game: design the bridge, prove it works on simulators, then let physics do the energy-saving flex when the silicon arrives.